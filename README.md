# ğŸ“š Document TOC Parser

[![Python Version](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Platform](https://img.shields.io/badge/platform-Windows%20%7C%20Linux%20%7C%20macOS-lightgrey.svg)]()

**æ™ºèƒ½æ–‡æ¡£ç›®å½•æå–å™¨** - å°†å†—é•¿çš„æŠ¥å‘Šä¹¦è‡ªåŠ¨ç»“æ„åŒ–ï¼Œè®© AI èƒ½å¤Ÿé«˜æ•ˆç†è§£å’Œå¤„ç†å¤§å‹æ–‡æ¡£ã€‚

[English](#) | [ç®€ä½“ä¸­æ–‡](#)

---

## ğŸ¯ æ ¸å¿ƒä»·å€¼

åœ¨å¤„ç†å¤§å‹æŠ¥å‘Šã€æŠ€æœ¯æ–‡æ¡£æ—¶ï¼Œä½ æ˜¯å¦é‡åˆ°è¿‡è¿™äº›é—®é¢˜ï¼š

âŒ **æ–‡æ¡£å¤ªé•¿** - ä¹ä¸‡å­—çš„ç¯è¯„æŠ¥å‘Šï¼ŒAI æ ¹æœ¬å¤„ç†ä¸äº†  
âŒ **ç»“æ„æ··ä¹±** - å‡ ç™¾é¡µçš„å†…å®¹ï¼Œä¸çŸ¥é“ä»å“ªå¼€å§‹åˆ†æ  
âŒ **æˆæœ¬å¤ªé«˜** - å…¨æ–‡å–‚ç»™ AIï¼ŒToken æ¶ˆè€—æƒŠäºº  
âŒ **æ•ˆæœä¸ä½³** - AI åœ¨é•¿æ–‡æœ¬ä¸­å®¹æ˜“é—æ¼å…³é”®ä¿¡æ¯  

### âœ… TOC Parser çš„è§£å†³æ–¹æ¡ˆ

```
ğŸ“„ å¤§å‹æ–‡æ¡£ (90,000å­—)
    â†“
ğŸ“‹ è‡ªåŠ¨æå–ç›®å½•ç»“æ„
    â†“
ğŸ—‚ï¸ ç»“æ„åŒ–JSONè¾“å‡º
    â†“
ğŸ¤– æŒ‰ç« èŠ‚ç²¾å‡†æŠ•å–‚AI
```

**æ ¸å¿ƒä¼˜åŠ¿ï¼š**
- ğŸš€ **æé«˜æ•ˆç‡**ï¼šå°† 90,000 å­—æ–‡æ¡£åˆ†è§£ä¸º 50+ ä¸ªå¯ç®¡ç†çš„ç« èŠ‚
- ğŸ’° **é™ä½æˆæœ¬**ï¼šæŒ‰éœ€å¤„ç†ï¼ŒToken ä½¿ç”¨é‡å‡å°‘ 80%
- ğŸ¯ **æå‡ç²¾åº¦**ï¼šAI ä¸“æ³¨äºç‰¹å®šç« èŠ‚ï¼Œå›ç­”æ›´å‡†ç¡®
- ğŸ“Š **å®Œæ•´æ´å¯Ÿ**ï¼šä¿ç•™æ–‡æ¡£å±‚çº§ç»“æ„ï¼Œä¸ä¸¢å¤±ä¸Šä¸‹æ–‡

---

## ğŸŒŸ åº”ç”¨åœºæ™¯

### 1ï¸âƒ£ **AI æ–‡æ¡£é—®ç­”ç³»ç»Ÿ**
```python
# æå–ç›®å½• â†’ ç”¨æˆ·æé—® â†’ å®šä½ç›¸å…³ç« èŠ‚ â†’ AI ç²¾å‡†å›ç­”
parser = TOCParser("æŠ€æœ¯æŠ¥å‘Š.docx")
toc = parser.parse()

# ç”¨æˆ·é—®ï¼š"ç¬¬ä¸‰ç« è®²äº†ä»€ä¹ˆï¼Ÿ"
chapter_3 = find_chapter(toc, "3")
response = ai.ask(chapter_3['content'])
```

**é€‚ç”¨äºï¼š**
- ğŸ“– æ³•å¾‹åˆåŒåˆ†æ
- ğŸ—ï¸ å·¥ç¨‹æŠ€æœ¯æŠ¥å‘Š
- ğŸ“Š ç ”ç©¶è®ºæ–‡æ£€ç´¢
- ğŸ“‹ ä¼ä¸šè§„ç« åˆ¶åº¦æŸ¥è¯¢

### 2ï¸âƒ£ **å¤§æ–‡æ¡£è‡ªåŠ¨æ‘˜è¦**
```python
# é€ç« ç”Ÿæˆæ‘˜è¦ï¼Œæœ€åæ±‡æ€»
for chapter in toc['chapters']:
    summary = ai.summarize(chapter['content'])
    chapter['summary'] = summary

# ç”Ÿæˆå®Œæ•´æŠ¥å‘Šæ‘˜è¦
full_summary = ai.merge_summaries(all_summaries)
```

**é€‚ç”¨äºï¼š**
- ğŸ“ ä¼šè®®çºªè¦å¿«é€Ÿç”Ÿæˆ
- ğŸ“‘ å­¦æœ¯è®ºæ–‡ç»¼è¿°
- ğŸ“Š å¸‚åœºè°ƒç ”æŠ¥å‘Šæç‚¼
- ğŸ” å°½èŒè°ƒæŸ¥æ–‡æ¡£åˆ†æ

### 3ï¸âƒ£ **æ–‡æ¡£æ™ºèƒ½å¯¹æ¯”**
```python
# å¯¹æ¯”æ–°æ—§ç‰ˆæœ¬å˜åŒ–
old_toc = TOCParser("2023ç‰ˆæŠ¥å‘Š.docx").parse()
new_toc = TOCParser("2024ç‰ˆæŠ¥å‘Š.docx").parse()

changes = compare_structure(old_toc, new_toc)
# â†’ "ç¬¬3.2èŠ‚æ–°å¢ï¼Œç¬¬5.1èŠ‚åˆ é™¤"
```

**é€‚ç”¨äºï¼š**
- ğŸ“„ åˆåŒç‰ˆæœ¬å˜æ›´è¿½è¸ª
- ğŸ“š æ ‡å‡†æ–‡æ¡£ä¿®è®¢å¯¹æ¯”
- ğŸ”„ SOP æµç¨‹æ›´æ–°æ£€æŸ¥

### 4ï¸âƒ£ **çŸ¥è¯†åº“æ„å»º**
```python
# æ‰¹é‡å¤„ç†ä¼ä¸šæ–‡æ¡£åº“
for doc in document_library:
    toc = TOCParser(doc).parse()
    knowledge_base.index(toc)

# æ”¯æŒç« èŠ‚çº§åˆ«çš„è¯­ä¹‰æœç´¢
results = knowledge_base.search("ç¯å¢ƒä¿æŠ¤æªæ–½")
```

**é€‚ç”¨äºï¼š**
- ğŸ¢ ä¼ä¸šçŸ¥è¯†ç®¡ç†ç³»ç»Ÿ
- ğŸ“š æ•°å­—å›¾ä¹¦é¦†å»ºè®¾
- ğŸ“ åœ¨çº¿å­¦ä¹ å¹³å°
- ğŸ”¬ ç§‘ç ”èµ„æ–™ç®¡ç†

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/Tsetmefree/toc-parser.git
cd toc-parser



**ä¾èµ–åŒ…ï¼š**
```
python-docx>=0.8.11
pdfplumber>=0.9.0
pywin32>=305  # å¯é€‰ï¼Œæ”¯æŒDOCæ ¼å¼ï¼ˆä»…Windowsï¼‰
```

### åŸºç¡€ä½¿ç”¨

```python
from toc_parser import TOCParser

# 1. è§£ææ–‡æ¡£
parser = TOCParser("ç¯è¯„æŠ¥å‘Š.docx")
result = parser.parse()

# 2. æŸ¥çœ‹ç»“æ„
parser.print_tree()

# 3. ä¿å­˜ä¸ºJSON
parser.save_json("output.json")
```

### å‘½ä»¤è¡Œä½¿ç”¨

```bash
# è§£æå¹¶æ˜¾ç¤º
python toc_parser.py æŠ¥å‘Šä¹¦.docx

# ä¿å­˜ä¸ºJSON
python toc_parser.py æŠ¥å‘Šä¹¦.docx --output result.json

# æ‰¹é‡å¤„ç†
python toc_parser.py *.docx --batch
```

---

## ğŸ“Š è¾“å‡ºæ ¼å¼

### æ ‘å½¢ç»“æ„ï¼ˆç”¨äºå±•ç¤ºï¼‰
```
ğŸ“ 1 æ€»åˆ™ (ç¬¬1é¡µ)
  ğŸ“„ 1.1 ç¼–åˆ¶ä¾æ® (ç¬¬1é¡µ)
  ğŸ“„ 1.2 ç¯å¢ƒåŠŸèƒ½åŒºåˆ’ (ç¬¬4é¡µ)
  ğŸ“„ 1.3 è¯„ä»·æ ‡å‡† (ç¬¬5é¡µ)
ğŸ“ 2 å·¥ç¨‹æ¦‚å†µä¸å·¥ç¨‹åˆ†æ (ç¬¬17é¡µ)
  ğŸ“„ 2.1 å·¥ç¨‹åœ°ç†ä½ç½® (ç¬¬17é¡µ)
  ğŸ“„ 2.2 é¡¹ç›®å»ºè®¾æ–¹æ¡ˆæ¯”é€‰ (ç¬¬17é¡µ)
```

### JSONç»“æ„ï¼ˆç”¨äºç¨‹åºå¤„ç†ï¼‰
```json
{
  "success": true,
  "metadata": {
    "filename": "æŠ¥å‘Šä¹¦.docx",
    "total_sections": 58,
    "level_1_count": 8,
    "level_2_count": 50
  },
  "toc": [
    {
      "number": "1",
      "title": "æ€»åˆ™",
      "page": 1,
      "level": 1,
      "children": [
        {
          "number": "1.1",
          "title": "ç¼–åˆ¶ä¾æ®",
          "page": 1,
          "level": 2,
          "children": []
        }
      ]
    }
  ],
  "flat_list": [
    {"number": "1", "title": "æ€»åˆ™", "page": 1, "level": 1},
    {"number": "1.1", "title": "ç¼–åˆ¶ä¾æ®", "page": 1, "level": 2}
  ]
}
```

---

## ğŸ¨ ç‰¹æ€§äº®ç‚¹

### âœ¨ æ™ºèƒ½è¯†åˆ«
- ğŸ”¢ **å¤šç§ç¼–å·æ ¼å¼**ï¼šæ”¯æŒ `1.` `1.1` `1.1.1` ç­‰å¤šçº§ç¼–å·
- ğŸ“ **çµæ´»åˆ†éš”ç¬¦**ï¼šè‡ªåŠ¨è¯†åˆ« Tabã€å¤šç©ºæ ¼ç­‰åˆ†éš”æ–¹å¼
- ğŸ¯ **ç²¾å‡†å®šä½**ï¼šè‡ªåŠ¨æ£€æµ‹"ç›®å½•"èµ·æ­¢ä½ç½®

### ğŸ“ æ ¼å¼æ”¯æŒ
| æ ¼å¼ | æ”¯æŒç¨‹åº¦ | è¯´æ˜ |
|------|---------|------|
| `.docx` | âœ… å®Œæ•´æ”¯æŒ | Word 2007+ æ ¼å¼ |
| `.doc` | âœ… æ”¯æŒ | éœ€è¦ Windows + Office |
| `.pdf` | âœ… å®Œæ•´æ”¯æŒ | åŸºäºæ–‡æœ¬çš„ PDF |

### ğŸ›¡ï¸ é²æ£’æ€§
- âœ… è‡ªåŠ¨è¿‡æ»¤ä¸´æ—¶æ–‡ä»¶ï¼ˆ`~$xxx.docx`ï¼‰
- âœ… å¼‚å¸¸å¤„ç†å®Œå–„ï¼Œä¸ä¼šä¸­æ–­æ‰¹é‡ä»»åŠ¡
- âœ… æ”¯æŒä¸è§„èŒƒç›®å½•æ ¼å¼çš„å®¹é”™å¤„ç†

---

## ğŸ’¡ è¿›é˜¶ç”¨æ³•

### ä¸ LangChain é›†æˆ

```python
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS

# 1. æå–ç›®å½•
toc = TOCParser("report.docx").parse()

# 2. æŒ‰ç« èŠ‚æ„å»ºå‘é‡åº“
for chapter in toc['flat_list']:
    text = extract_chapter_content(chapter)
    chunks = text_splitter.split_text(text)
    
    # æ·»åŠ å…ƒæ•°æ®
    metadata = {
        "chapter": chapter['number'],
        "title": chapter['title'],
        "page": chapter['page']
    }
    
    vectorstore.add_texts(chunks, metadatas=[metadata]*len(chunks))

# 3. ç« èŠ‚çº§åˆ«çš„æ£€ç´¢å¢å¼º
query = "ç¯å¢ƒä¿æŠ¤æªæ–½æœ‰å“ªäº›ï¼Ÿ"
docs = vectorstore.similarity_search(query, k=3)
```

### ä¸ GPT API é…åˆ

```python
import openai

def chunk_analysis(toc_json, model="gpt-4"):
    """é€ç« åˆ†æï¼Œé¿å…è¶…å‡º Token é™åˆ¶"""
    
    results = []
    for chapter in toc_json['toc']:
        prompt = f"""
        è¯·åˆ†æä»¥ä¸‹ç« èŠ‚å†…å®¹ï¼š
        ç« èŠ‚ï¼š{chapter['number']} {chapter['title']}
        é¡µç ï¼šç¬¬{chapter['page']}é¡µ
        
        å†…å®¹ï¼š{chapter['content']}
        
        è¯·æ€»ç»“è¦ç‚¹ï¼š
        """
        
        response = openai.ChatCompletion.create(
            model=model,
            messages=[{"role": "user", "content": prompt}]
        )
        
        results.append({
            "chapter": chapter['number'],
            "summary": response.choices[0].message.content
        })
    
    return results
```

---

## ğŸ”§ é…ç½®é€‰é¡¹

åˆ›å»º `config.json` è‡ªå®šä¹‰è¯†åˆ«è§„åˆ™ï¼š

```json
{
  "toc_patterns": [
    "^(\\d+)\\.\\s+([^\\t]+?)\\s+(\\d+)$",
    "^(\\d+\\.\\d+)\\s+([^\\t]+?)\\s+(\\d+)$"
  ],
  "toc_start_keywords": ["ç›®å½•", "ç›®  å½•", "CONTENTS"],
  "toc_end_keywords": ["é™„å›¾", "é™„ä»¶", "é™„è¡¨", "æ­£æ–‡"],
  "ignore_files": ["~$*", ".*"],
  "output_format": "json"
}
```

---



## ğŸ¤ è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿å„ç§å½¢å¼çš„è´¡çŒ®ï¼

### æäº¤ Bug
- è¯·åœ¨ [Issues](https://github.com/Tsetmefree/toc-parser/issues) ä¸­è¯¦ç»†æè¿°é—®é¢˜
- é™„ä¸Šæ ·æœ¬æ–‡æ¡£ï¼ˆè„±æ•åï¼‰å’Œé”™è¯¯æ—¥å¿—

### åŠŸèƒ½å»ºè®®
- åœ¨ Issues ä¸­æ‰“ä¸Š `enhancement` æ ‡ç­¾
- è¯´æ˜ä½¿ç”¨åœºæ™¯å’ŒæœŸæœ›æ•ˆæœ

### Pull Request
1. Fork æœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

---

## ğŸ™ è‡´è°¢

- [python-docx](https://github.com/python-openxml/python-docx) - DOCX æ–‡ä»¶å¤„ç†
- [pdfplumber](https://github.com/jsvine/pdfplumber) - PDF æ–‡æœ¬æå–
- æ‰€æœ‰è´¡çŒ®è€…å’Œä½¿ç”¨è€…

---


## ğŸŒŸ Star History

å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª â­ï¸ Starï¼

[![Star History Chart](https://api.star-history.com/svg?repos=yourusername/toc-parser&type=Date)](https://star-history.com/#yourusername/toc-parser&Date)

---

<div align="center">
Made with â¤ï¸ by developers who hate reading long documents
</div>
